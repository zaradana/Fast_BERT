{
    "train_batch_size":64,
    "max_seq_length":128,
    "warmup_steps":10,
    "epochs":1,
    "lr":0.5,
    "adam_epsilon":0.2,
    "use_fast_tokenizer": "True",
    "model_type":"bert",
    "grad_accumulation_steps":1,
    "logging_steps":1,
    "save_steps":50,
    "model_name":"distilbert-base-multilingual-cased",
    "do_lower_case": "False"
}